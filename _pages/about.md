---
permalink: /
title: "Leonardo Cotta"
excerpt: "About"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

### About

I'm a postdoc fellow at the [Vector Institute](https://vectorinstitute.ai/), where I'm hosted by [Chris J. Maddison](https://www.cs.toronto.edu/~cmaddis/). I received my PhD degree in computer science at Purdue University, where I was advised by [Bruno Ribeiro](https://www.cs.purdue.edu/homes/ribeirob/). Before that ---in a distant and happy land--- I was a BSc student (also in CS) at UFMG, Brazil. During my time as an undergrad I worked with distributed algorithms (at UFMG) and Quantum Computing Theory (at University of Calgary).

### Research

I'm broadly interested in statistical and causal machine learning. More specifically, I study the interplay between symmetries, computation, and learning. How can we leverage these concepts to build better, and practical, machine learning methodology? Most of the problems I like to work with rise from the (possible) combinatorial structure of data. Next, you can see a few perspectives on this. 

**Invariant representations of combinatorial data.** Representations that incorporate the inherent invariances of combinatorial data result in learning algorithms with better generalization capabilities. I am interested in designing representations of graphs, sets and posets that both incorporate the natural invariances present in the data and are expressive enough to distinguish different combinatorial objects. Finally, I'm also interested in better quantifying the effect of an specific invariance feature in the generalization performance of a learning algorithm.

**Combinatorial representations of combinatorial data.** A combinatorial object can be represented with its underlying combinatorial decompositions. For instance, we can describe a graph with its subgraphs, a set with its subsets or a poset with its chains. I'm interested in understanding how to represent each of these substructures and how they can be combined to represent the entire combinatorial object. We have already shown that for certain graph tasks such decompositions can lead to more powerful and robust learning algorithms. I want to understand in general what is the role of combinatorial decomposition in supervised learning, *i.e.* by how much generalization is impacted and under what conditions.

**Learning to answer causal queries with combinatorial data.** In real-world systems we are often presented with combinatorial data , *e.g.* networks, and can perform experiments to observe new outcomes. How can we *learn* to answer causal queries from such experiments? What does learning even mean in this context? What are possible causal models for combinatorial and invariant data? What is the role of invariant representations here? These are all new and exciting questions I have been working on over the last year or so. Check out our recent [pre-print](https://arxiv.org/abs/2302.01198) for some answers.

**Applications.** Recommender systems and life sciences.

### News

<span style="background-color: #FFFF00">I started as a Postdoc Fellow at the Vector Institute! If you're ever in Toronto and want to talk research, drop me a line! </span>

<span style="background-color: #FFFF00">I will be mentoring a project on poset representation learning at [LOGML](https://www.logml.ai/) this year! Apply! </span>

<span style="background-color: #FFFF00">Accepted at NeurIPS 2021:</span>

**Reconstruction for Powerful Graph Representations**

Leonardo Cotta, Christopher Morris, Bruno Ribeiro

<span style="background-color: #FFFF00">I'll be on an (remote) internship this summer (2021) at Intel Labs!</span>

<span style="background-color: #FFFF00">Accepted at NeurIPS 2020:</span>

**Unsupervised Joint $k$-node Graph Representations with Compositional Energy-Based Models**
