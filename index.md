## About

Hi! I'm a PhD candidate in Computer Science @ Purdue University, where I'm advised by Prof. [Bruno Ribeiro](https://www.cs.purdue.edu/homes/ribeirob/). Previously, I was a B.Sc. student (also in CS) @ UFMG, Brazil.

Previously, I worked with distributed algorithms (@ UFMG) and Quantum Computing Theory (@ University of Calgary).

My CV (October/2021) is [here](CV-2021.pdf)

The best way to read about my research is going to [my GoogleScholar profile](https://goo.gl/zrNQue). I try to maintain my repositories here at Github as up to date as possible. If you don't find what you are looking for here or if you have any extra questions about my research, drop me a line!


## Research

I'm interested in understanding how machines can learn to reason with combinatorial and invariant data, *e.g.* graphs, sets and posets. In this context, I often use tools from statistical machine learning, causal inference, Monte Carlo sampling, combinatorics and abstract algebra.

**Invariant representations of combinatorial data.** Representations that incorporate the inherent invariances of combinatorial data result in learning algorithms with better generalization capabilities. I am interested in understanding how we can design representations of graphs, sets and posets that both incorporate the natural invariances present in the data and are expressive enough to distinguish different combinatorial objects. Finally, I'm also interested in better quantifying the effect of a specific invariance feature in the generalization performance of a learning algorithm.

**Combinatorial representations of combinatorial data.** A combinatorial object can be represented with one of its underlying combinatorial decompositions. For instance, we can describe a graph with its subgraphs, a set with its subsets or a poset with its chains. I'm interested in understanding how to represent each of this substructures and how they can be combined to represent the entire combinatorial object. We have already shown that for certain graph tasks such decompositions can lead to more powerful and robust learning algorithms. I want to understand in general what is the role of combinatorial decomposition in learning, *i.e.* by how much generalization is impacted and under what conditions.

**Learning to answer causal queries with combinatorial data.** In real-world systems we are often presented with observed combinatorial data , *e.g.* networks, and can perform experiments to observe more outcomes. How can we *learn* to answer causal queries from such experiments? What does learning even mean in this context? What are possible causal models for combinatorial and invariant data? What is the role of invariant representations here? These are all new and exciting questions I have been working on over the last months.

**Applications.** Recommender systems, bioinformatics, ecology, computer networks and sports analytics.

## News

<span style="background-color: #FFFF00">I will be mentoring a project on poset representation learning at [LOGML](https://www.logml.ai/) this year! Apply! </span>

<span style="background-color: #FFFF00">Accepted at NeurIPS 2021:</span>

**Reconstruction for Powerful Graph Representations**

Leonardo Cotta, Christopher Morris, Bruno Ribeiro

<span style="background-color: #FFFF00">I'll be on an (remote) internship this summer (2021) at Intel Labs!</span>

<span style="background-color: #FFFF00">Accepted at NeurIPS 2020:</span>

**Unsupervised Joint <img src="https://render.githubusercontent.com/render/math?math=k">-node Graph Representations with Compositional Energy-Based Models**

Leonardo Cotta, Carlos H.C. Teixeira, Ananthram Swami, Bruno Ribeiro

## Contact

cotta [at] purdue [dot] edu

[Follow me on Twitter](https://twitter.com/cottascience)!

## Service

NeurIPS 2022 TPC

AAAI 2021 TPC

SDM 2021 TPC
